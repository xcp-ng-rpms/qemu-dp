Avoid repeated memory allocation in xen_disk

From: Tim Smith <tim.smith@citrix.com>

xen_disk currently allocates memory to hold the data for each ioreq
as that ioreq is used, and frees it afterwards. Because it requires
page-aligned blocks, this interacts poorly with non-page-aligned
allocations and balloons the heap.

Instead, allocate the maximum possible requirement, which is
BLKIF_MAX_SEGMENTS_PER_REQUEST pages (currently 11 pages) when
the ioreq is created, and keep that allocation until it is destroyed.
Since the ioreqs themselves are re-used via a free list, this
should actually improve memory usage.
---
 hw/block/xen_disk.c |   11 +++++------
 1 file changed, 5 insertions(+), 6 deletions(-)

diff --git a/hw/block/xen_disk.c b/hw/block/xen_disk.c
index fc6fb3d793..f68c75887a 100644
--- a/hw/block/xen_disk.c
+++ b/hw/block/xen_disk.c
@@ -147,7 +147,6 @@ static void ioreq_reset(struct ioreq *ioreq)
     memset(ioreq->refs, 0, sizeof(ioreq->refs));
     ioreq->prot = 0;
     memset(ioreq->page, 0, sizeof(ioreq->page));
-    ioreq->pages = NULL;
 
     ioreq->aio_inflight = 0;
     ioreq->aio_errors = 0;
@@ -210,6 +209,10 @@ static struct ioreq *ioreq_start(struct XenBlkDev *blkdev)
         /* allocate new struct */
         ioreq = g_malloc0(sizeof(*ioreq));
         ioreq->blkdev = blkdev;
+        /* We cannot need more pages per ioreq than this, and we do re-use ioreqs,
+         * so allocate the memory once here, to be freed in blk_free() when the
+         * ioreq is freed. */
+        ioreq->pages = qemu_memalign(XC_PAGE_SIZE, BLKIF_MAX_SEGMENTS_PER_REQUEST * XC_PAGE_SIZE);
         blkdev->requests_total++;
         qemu_iovec_init(&ioreq->v, BLKIF_MAX_SEGMENTS_PER_REQUEST);
     } else {
@@ -335,7 +338,6 @@ static void ioreq_unmap(struct ioreq *ioreq)
                           strerror(errno));
         }
         ioreq->blkdev->cnt_map -= ioreq->num_unmap;
-        ioreq->pages = NULL;
     } else {
         for (i = 0; i < ioreq->num_unmap; i++) {
             if (!ioreq->page[i]) {
@@ -500,8 +502,6 @@ static void ioreq_free_copy_buffers(struct ioreq *ioreq)
     for (i = 0; i < ioreq->v.niov; i++) {
         ioreq->page[i] = NULL;
     }
-
-    qemu_vfree(ioreq->pages);
 }
 
 static int ioreq_init_copy_buffers(struct ioreq *ioreq)
@@ -512,8 +512,6 @@ static int ioreq_init_copy_buffers(struct ioreq *ioreq)
         return 0;
     }
 
-    ioreq->pages = qemu_memalign(XC_PAGE_SIZE, ioreq->v.niov * XC_PAGE_SIZE);
-
     for (i = 0; i < ioreq->v.niov; i++) {
         ioreq->page[i] = ioreq->pages + i * XC_PAGE_SIZE;
         ioreq->v.iov[i].iov_base = ioreq->page[i];
@@ -1433,6 +1431,7 @@ static int blk_free(struct XenDevice *xendev)
         ioreq = QLIST_FIRST(&blkdev->freelist);
         QLIST_REMOVE(ioreq, list);
         qemu_iovec_destroy(&ioreq->v);
+        qemu_vfree(ioreq->pages);
         g_free(ioreq);
     }
 
